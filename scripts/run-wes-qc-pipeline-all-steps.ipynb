{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run WES-QC pipeline\n",
    "\n",
    "This Jupyter notebook runs all steps for WES-QC pipeline.\n",
    "It supports both running in an SSH mode (on a local machine with remote access to the cluster) or directly from the remote cluster head node.\n",
    "\n",
    "For details explaining the steps, see the documentation in the `docs/wes-qc-hail.md`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How to run the notebook\n",
    "\n",
    "### Run with jupyter notebook on the local machine\n",
    "\n",
    "1. First, prepare a python environment with a Jupyter server. There is no additional dependency to install.\n",
    "A quick way to do this is to install `uv` - `curl -LsSf https://astral.sh/uv/install.sh | sh`, see [uv](https://astral.sh/uv/) for more details.\n",
    "Then run `uv add jupyterlab`, the `jupyter` executable will be installed in `.venv/bin/jupyter`.\n",
    "\n",
    "2. Then, set up an SSH connection to the cluster, for example, if the cluster IP address is `172.27.1.1`, add the following to your `~/.ssh/config` file:\n",
    "```\n",
    "Host wes\n",
    "    HostName 172.27.1.1\n",
    "    User ubuntu\n",
    "    IdentityFile ~/.ssh/id_rsa\n",
    "```\n",
    "id_rsa is the private key for accessing the cluster when the cluster was created.\n",
    "\n",
    "3. Then, in the python environment, run the following command to start the notebook:\n",
    "```\n",
    "jupyter notebook scripts/run-wes-qc-pipeline-all-steps.ipynb\n",
    "```\n",
    "or if you are using VSCode, you can open the notebook directly in VSCode and set the Python interpreter to the one with the Jupyter notebook installed.\n",
    "\n",
    "4. Follow the instructions in the notebook to run the steps.\n",
    "Note that the variable `jupyter_notebook_on_cluster` needs to be set to `False` in the notebook.\n",
    "\n",
    "**Warning:** if you run the notebook in SSH mode, your computer runs a local jupyter server to execute commands and track the progress.\n",
    "If your computer loses connection to the cluster head node due to any network issues (or, for example, when your computer goes to sleep), the data processing terminates.\n",
    "\n",
    "\n",
    "### Run with jupyter notebook on the cluster\n",
    "\n",
    "1. First, prepare a python environment with Jupyter notebook installed, same as above.\n",
    "See the above [Run with jupyter notebook on the local machine](#run-with-jupyter-notebook-on-the-local-machine) section for more details.\n",
    "\n",
    "2. Then, start a jupyter notebook server on the cluster:\n",
    "```\n",
    "jupyter notebook --no-browser --port=8889 scripts/run-wes-qc-pipeline-all-steps.ipynb\n",
    "```\n",
    "**Note:** for clusters created by the Sanger `osdataproc` utitity you cannot use the default port 8888 because it is already used by the built-in notebook server on the master node.\n",
    "The built-in notebook server on the master node is not suitable because its Spark will claim all the resources and the pipeline will not be able to run.\n",
    "\n",
    "If you are using VSCode, you can open the notebook directly in VSCode and set the python interpreter to the system python.\n",
    "\n",
    "3. Follow the instructions in the notebook to run the pipeline.\n",
    "Note that the variable `jupyter_notebook_on_cluster` needs to be set to `True` in the notebook.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare to run\n",
    "\n",
    "### Define several service functions"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set this to `True` if you want to execute this jupyter notebook on the cluster head node.\n",
    "jupyter_notebook_on_cluster = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# The small function to run command either via SSH (for local run)\n",
    "# or directly (when the notebook is on cluster)\n",
    "def run_cmd(cmd):\n",
    "    if jupyter_notebook_on_cluster:\n",
    "        !{cmd}\n",
    "    else:\n",
    "        !ssh -o StrictHostKeyChecking=no wes \"{cmd}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function that runs a series of scripts\n",
    "def run_step_scripts(step_folder, scripts):\n",
    "    for script, prefix in scripts.items():\n",
    "        print(\"=\" * 120 + \"\\n\")\n",
    "        logger.info(f\"Running {script}\")\n",
    "        cmd = f\"cd {path_to_wes} && ./scripts/hlrun_local --prefix={prefix} {step_folder}/{script}\"\n",
    "        run_cmd(cmd)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the path to the wes-qc directory\n",
    "\n",
    "All WES-QC repo should be located on the cluster head node).\n",
    "Make sure the config/inputs.yaml symlinks to the correct dataset config."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "path_to_wes = \"/lustre/scratch126/teams/hgi/gz3/wes_qc_pycharm\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Check Hail is working on the cluster"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "python = \"/home/ubuntu/venv/bin/python\"\n",
    "cmd = (\n",
    "    f'cd {path_to_wes}; {python} -c \\\\\"import hail as hl; hl.init()\\\\\" '\n",
    "    if not jupyter_notebook_on_cluster\n",
    "    else f'cd {path_to_wes}; {python} -c \"import hail as hl; hl.init()\" '\n",
    ")\n",
    "run_cmd(cmd)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Processing your data\n",
    "\n",
    "### How to configure run steps\n",
    "To configure run steps, you only need to set up a dictionary describing the step.\n",
    "Dictionary key is the command to run (with arguments if they are required), the value is the name of the log file.\n",
    "This notebook automatically collects all logs form the running steps.\n",
    "\n",
    "\n",
    "### Step 0 - Resource Preparation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "step0 = {\"1-import_1kg.py --all\": \"0-1-import-1kg\", \"2-generate-truthset-ht.py\": \"0-2-generate-truthset-ht\"}\n",
    "step0_folder = \"0-resource_preparation\"\n",
    "run_step_scripts(step0_folder, step0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1 - Import Data"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "step1 = {\n",
    "    \"1-import_gatk_vcfs_to_hail.py\": \"1-1-import_gatk_vcfs_to_hail\",\n",
    "    \"2-import_annotations.py\": \"1-2-import_annotations\",\n",
    "    \"3-validate-gtcheck.py\": \"1-3-validate-gtcheck\",\n",
    "    \"4-mutation-spectra_preqc.py\": \"1-4-mutation-spectra_preqc\",\n",
    "}\n",
    "step1_folder = \"1-import_data\"\n",
    "run_step_scripts(step1_folder, step1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Sample QC\n",
    "\n",
    "The SampleQC step is pretty automated and mostly produced acceptable results even with the default set of parameters.\n",
    "However, we strongly advise you to refer to the documentation, review the metrics plots and tune filtering thresholds if necessary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "step2 = {\n",
    "    \"1-hard_filters_sex_annotation.py\": \"2-1-hard-filters-sex-annotation\",\n",
    "    \"2-prune_related_samples.py\": \"2-2-prune-related-samples\",\n",
    "    \"3-population_pca_prediction.py --all\": \"2-3-population-pca-prediction\",\n",
    "    \"4-find_population_outliers.py\": \"2-4-find-population-outliers\",\n",
    "    \"5-filter_fail_sample_qc.py\": \"2-5-filter-fail-sample-qc\",\n",
    "}\n",
    "step2_folder = \"2-sample_qc\"\n",
    "run_step_scripts(step2_folder, step2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**After this step, you must review the graphs generated by the step 2.4 and check the total number of survived/filtered samples.**\n",
    "\n",
    "If necessary, tune the sample filtering parameters in the config file. Please refer to the manual for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Variant QC \n",
    "The first part of the VariantQC step - before training the random forest model.\n",
    "Don't forget to set the RF model ID you want."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_id = \"test-run-model\"\n",
    "\n",
    "step3_1 = {\n",
    "    \"1-split_and_family_annotate.py --all\": \"3-1-split_and_family_annotate\",\n",
    "    \"2-create_rf_ht.py\": \"3-2-create-rf-ht\",\n",
    "    f\"3-train_rf.py --manual-model-id {model_id}\": \"3-3-train-rf\",\n",
    "}\n",
    "step3_folder = \"3-variant_qc\"\n",
    "run_step_scripts(step3_folder, step3_1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "At this point we need to change the model run ID in the config file"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "yaml_file = \"config/inputs.yaml\"\n",
    "cmd = f\"sed --follow-symlinks -i 's/rf_model_id:.*/rf_model_id: {model_id}/' {path_to_wes}/{yaml_file}\"\n",
    "run_cmd(cmd)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the second part of the VariantQC. Please ferer to the docs (`docs/wes-qc-hail.md`) for details.\n",
    "\n",
    "After finishing the variant QC process, you need to review the results and choose hardfilters for the genotype filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "step3_2 = {\n",
    "    \"4-apply_rf.py\": \"3-4-apply-rf\",\n",
    "    \"5-annotate_ht_after_rf.py\": \"3-5-annotate-ht-after-rf\",\n",
    "    \"6-rank_and_bin.py\": \"3-6-rank-and-bin\",\n",
    "    \"7-plot_rf_output.py\": \"3-7-plot-rf-output\",\n",
    "}\n",
    "step3_folder = \"3-variant_qc\"\n",
    "run_step_scripts(step3_folder, step3_2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**At this point you must review the resulting plots to choose the correct bins for the first step of GenotypeQC.**\n",
    "Please refer to the main manual for instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Genotype QC\n",
    "\n",
    "To perform genotype QC, you need to determine the best combination of hard filters,\n",
    "to save \"good\" variations as much as possible,\n",
    "and get rid of all \"bad\" variants and genotypes at the same time.\n",
    "\n",
    "The first script of the genotype QC helps you to analyze different combinations of hard filters\n",
    "and choose optimal values.\n",
    "\n",
    "Please refer to the docs for details."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "step4_1 = {\n",
    "    \"1-compare_hard_filter_combinations.py --all\": \"4-1-compare-hard-filter-combinations\",\n",
    "}\n",
    "step4_folder = \"4-genotype_qc\"\n",
    "run_step_scripts(step4_folder, step4_1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this step, you MUST review and analyze the results to choose correct values for hardfilter combinations**.\n",
    "The values for the public datasets are not suitable for your data.\n",
    "Please refer to the docs for details: `docs/wes-qc-hail.md`.\n",
    "\n",
    "After choosing the hard filters, you can run the last part of the data processing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "step4_2 = {\n",
    "    \"2-apply_range_of_hard_filters.py\": \"4-2-apply-range-of-hard-filters\",\n",
    "    \"3a-export_vcfs_range_of_hard_filters.py\": \"4-3a-export-vcfs-range-of-hard-filters\",\n",
    "    \"3b-export_vcfs_stringent_filters.py\": \"4-3b-export-vcfs-stringent-filters\",\n",
    "    \"5-mutation-spectra_afterqc.py\": \"4-5-mutation-spectra_afterqc\",\n",
    "}\n",
    "step4_folder = \"4-genotype_qc\"\n",
    "run_step_scripts(step4_folder, step4_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
