{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run WES-QC pipeline\n",
    "\n",
    "This Jupyter notebook runs all steps for WES-QC pipeline\n",
    "\n",
    "It supports both running form a local machine or from remote cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up jupyter notebook to run on local or on the cluster\n",
    "By default, the Jupyter notebook is running on the local machine, or any machine that has SSH access to the cluster\n",
    "\n",
    "For local run you need to have the `wes` hostname, containing the username and IP address \n",
    "of your cluster. You can specify it in the SSH config like this:\n",
    "\n",
    "```\n",
    "Host wes\n",
    "    HostName 172.27.1.1\n",
    "    User ubuntu\n",
    "    IdentityFile ~/.ssh/id_rsa\n",
    "``` \n",
    "\n",
    "Set it to `True` if you want to run this jupyter notebook on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_notebook_on_cluster = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the path to the wes-qc directory\n",
    "\n",
    "All WES-QC repo should be located on the machine (cluster).\n",
    "Make sure the config/inputs.yaml symlinks to the correct dataset config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_wes = \"/home/ubuntu/temprun/wes_qc\"\n",
    "path_to_wes = \"/lustre/scratch126/teams/hgi/eh19/wes-qc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to add steps to stages\n",
    "\n",
    "The notebook runs all steps for a specific stage using the step dictionary,\n",
    "containing the script step name (withh all command-line parameters if they are needed)\n",
    "and a corresponding log file name.\n",
    "\n",
    "To add/modify a step, change this directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The small function to run command either via SSH (for local run)\n",
    "# or directly (when the notebook is on cluster\n",
    "def run_cmd(cmd):\n",
    "    if jupyter_notebook_on_cluster:\n",
    "        !{cmd}\n",
    "    else:\n",
    "        !ssh wes \"{cmd}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Hail is working on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "Running on Apache Spark version 3.5.3\n",
      "SparkUI available at http://spark-master:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.133-4c60fddb171a\n",
      "LOGGING: writing to /home/ubuntu/temprun/wes_qc/hail-20250106-1104-0.2.133-4c60fddb171a.log\n"
     ]
    }
   ],
   "source": [
    "python = \"/home/ubuntu/venv/bin/python\"\n",
    "cmd = (\n",
    "    f'cd {path_to_wes}; {python} -c \\\\\"import hail as hl; hl.init()\\\\\" '\n",
    "    if not jupyter_notebook_on_cluster\n",
    "    else f'cd {path_to_wes}; {python} -c \"import hail as hl; hl.init()\" '\n",
    ")\n",
    "run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 - Resource Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-06 11:04:58,797 - INFO - Running 1-import_1kg.py --all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "\n",
      "Running the job with spark-submit\n",
      "spark-submit 0-resource_preparation/1-import_1kg.py --all\n",
      "info: script_dir  /home/ubuntu/temprun/wes_qc/0-resource_preparation\n",
      "Loading config '/home/ubuntu/temprun/wes_qc/0-resource_preparation/../config/inputs.yaml', default\n",
      "=== Cleaning up temporary folder /lustre/scratch126/teams/hgi/gz3/public-dataset/tmp\n",
      "=== Initializing Hail ===\n",
      "Running on Apache Spark version 3.5.3\n",
      "SparkUI available at http://spark-master:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.133-4c60fddb171a\n",
      "LOGGING: writing to /home/ubuntu/temprun/wes_qc/hail-20250106-1105-0.2.133-4c60fddb171a.log\n",
      "Loading VCFs from file:///lustre/scratch126/teams/hgi/gz3/public-dataset/resources/mini_1000G\n",
      "2025-01-06 11:05:17.482 Hail: INFO: Reading table without type imputation1) / 1]\n",
      "  Loading field 'Sample name' as type str (not specified)\n",
      "  Loading field 'Sex' as type str (not specified)\n",
      "  Loading field 'Biosample ID' as type str (not specified)\n",
      "  Loading field 'Population code' as type str (not specified)\n",
      "  Loading field 'Population name' as type str (not specified)\n",
      "  Loading field 'Superpopulation code' as type str (not specified)\n",
      "  Loading field 'Superpopulation name' as type str (not specified)\n",
      "  Loading field 'Population elastic ID' as type str (not specified)\n",
      "  Loading field 'Data collections' as type str (not specified)\n",
      "Saving as hail mt to file:///lustre/scratch126/teams/hgi/gz3/public-dataset/matrixtables/kg_unprocessed.mt\n",
      "2025-01-06 11:05:19.746 Hail: INFO: scanning VCF for sortedness...  (0 + 1) / 1]\n",
      "2025-01-06 11:05:31.607 Hail: INFO: Coerced prefix-sorted VCF, requiring additional sorting within data partitions on each query.\n",
      "2025-01-06 11:06:33.737 Hail: INFO: wrote matrix table with 1004437 rows and 150 columns in 1 partition to file:///lustre/scratch126/teams/hgi/gz3/public-dataset/matrixtables/kg_unprocessed.mt\n",
      "2025-01-06 11:06:36.401 Hail: INFO: Reading table without type imputation1) / 1]\n",
      "  Loading field 'f0' as type str (user-supplied)\n",
      "  Loading field 'f1' as type int32 (user-supplied)\n",
      "  Loading field 'f2' as type int32 (user-supplied)\n",
      "  Loading field 'f3' as type str (user-supplied)\n",
      "2025-01-06 11:06:36.975 Hail: INFO: ld_prune: running local pruning stage with max queue size of 762601 variants\n",
      "2025-01-06 11:06:47.406 Hail: INFO: Coerced sorted dataset          (0 + 1) / 1]\n",
      "2025-01-06 11:07:05.490 Hail: INFO: wrote table with 6211 rows in 1 partition to file:///lustre/scratch126/teams/hgi/gz3/public-dataset/tmp/persist_Tablebzi0fpjRSc\n",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "step0 = {\"1-import_1kg.py --all\": \"0-1-import-1kg\"}\n",
    "for script, prefix in step0.items():\n",
    "    print(\"=\" * 120 + \"\\n\")\n",
    "    logger.info(f\"Running {script}\")\n",
    "    cmd = f\"cd {path_to_wes} && ./scripts/hlrun_local --prefix={prefix} 0-resource_preparation/{script}\"\n",
    "    run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = {\n",
    "    \"1-import_gatk_vcfs_to_hail.py\": \"1-1-import_gatk_vcfs_to_hail\",\n",
    "    \"2-import_annotations.py\": \"1-2-import_annotations\",\n",
    "}\n",
    "for script, prefix in step1.items():\n",
    "    print(\"=\" * 120 + \"\\n\")\n",
    "    logger.info(f\"Running {script}\")\n",
    "    cmd = f\"cd {path_to_wes} && ./scripts/hlrun_local --prefix={prefix} 1-import_data/{script}\"\n",
    "    run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Sample QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2 = {\n",
    "    # \"1-hard_filters_sex_annotation.py\": \"2-1-hard-filters-sex-annotation\",\n",
    "    # \"2-prune_related_samples.py\": \"2-2-prune-related-samples\",\n",
    "    \"3-population_pca_prediction.py --all\": \"2-3-population-pca-prediction\",\n",
    "    \"4-find_population_outliers.py\": \"2-4-find-population-outliers\",\n",
    "    \"5-filter_fail_sample_qc.py\": \"2-5-filter-fail-sample-qc\",\n",
    "}\n",
    "for script, prefix in step2.items():\n",
    "    print(\"=\" * 120 + \"\\n\")\n",
    "    logger.info(f\"Running {script}\")\n",
    "    cmd = f\"cd {path_to_wes}; ./scripts/hlrun_local --prefix={prefix} 2-sample_qc/{script}\"\n",
    "    run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Variant QC \n",
    "Run with the given model id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"test-run-model\"\n",
    "\n",
    "step3 = {\n",
    "    \"1-generate_truth_sets.py --all\": \"3-1-generate-truth-sets\",\n",
    "    \"2-create_rf_ht.py\": \"3-2-create-rf-ht\",\n",
    "    \"3-train_rf.py --manual-model-id {model_id}\": \"3-3-train-rf\",\n",
    "}\n",
    "for script, prefix in step3.items():\n",
    "    print(\"=\" * 120 + \"\\n\")\n",
    "    logger.info(f\"Running {script}\")\n",
    "    cmd = f\"cd {path_to_wes}; ./scripts/hlrun_local --prefix={prefix} 3-variant_qc/{script}\"\n",
    "    run_cmd(cmd)\n",
    "\n",
    "yaml_file = \"config/inputs.yaml\"\n",
    "cmd = f\"sed --follow-symlinks -i 's/rf_model_id:.*/rf_model_id: {model_id}/' {path_to_wes}/{yaml_file}\"\n",
    "run_cmd(cmd)\n",
    "\n",
    "step3 = {\n",
    "    \"4-apply_rf.py\": \"3-4-apply-rf\",\n",
    "    \"5-annotate_ht_after_rf.py\": \"3-5-annotate-ht-after-rf\",\n",
    "    \"6-rank_and_bin.py\": \"3-6-rank-and-bin\",\n",
    "    \"7-plot_rf_output.py\": \"3-7-plot-rf-output\",\n",
    "    \"8-select_thresholds.py --snv 80 --indel 60\": \"3-8-select-thresholds\",\n",
    "    \"9-filter_mt_after_variant_qc.py --snv 80 --indel 60\": \"3-9-filter-mt-after-variant-qc\",\n",
    "}\n",
    "for script, prefix in step3.items():\n",
    "    print(\"=\" * 120 + \"\\n\")\n",
    "    logger.info(f\"Running {script}\")\n",
    "    cmd = f\"cd {path_to_wes}; ./scripts/hlrun_local --prefix={prefix} 3-variant_qc/{script}\"\n",
    "    run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Genotype QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step4 = {\n",
    "    \"1-compare_hard_filter_combinations.py\": \"4-1-compare-hard-filter-combinations\",\n",
    "    \"2-apply_range_of_hard_filters.py\": \"4-2-apply-range-of-hard-filters\",\n",
    "    \"3a-export_vcfs_range_of_hard_filters.py\": \"4-3a-export-vcfs-range-of-hard-filters\",\n",
    "    \"3b-export_vcfs_stingent_filters.py\": \"4-3b-export-vcfs-stingent-filters\",\n",
    "}\n",
    "for script, prefix in step4.items():\n",
    "    print(\"=\" * 120 + \"\\n\")\n",
    "    logger.info(f\"Running {script}\")\n",
    "    cmd = f\"cd {path_to_wes}; ./scripts/hlrun_local --prefix={prefix} 4-genotype_qc/{script}\"\n",
    "    run_cmd(cmd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
